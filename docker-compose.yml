version: "3.8"

services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    env_file: .env
    ports:
      - "8000:8000"
    volumes:
      - "~/.cache/huggingface:/root/.cache/huggingface"
    ipc: host
    command: [
      # "Gemma is provided under and subject to the Gemma Terms of Use found at ai.google.dev/gemma/terms"
      "--model=gaunernst/gemma-3-12b-it-int4-awq",
      "--enable-reasoning",
      # Possible choices (required for enable): deepseek_r1, granite
      "--reasoning-parser=deepseek_r1", 
      # Specify the amount of CPU offload in GB
      "--cpu-offload-gb=2",  
      # Use ngram spec-dec... free performance
      "--speculative-config={\"method\": \"ngram\", \"num_speculative_tokens\": 5, \"prompt_lookup_max\": 4}",
      "--trust-remote-code",
      # Quantize KV cache to FP8 with 4b exp and 3b mantissa
      "--kv-cache-dtype=fp8_e4m3",
      # Use outlines for best JSON Schema support
      "--guided-decoding-backend=outlines",
      "--enable-prefix-caching",
      # Deterministic-ish outputs
      "--seed=666",
      "--override-generation-config={\"temperature\": 0}",  
      # Model context length. If unspecified, will be automatically derived from the model config.
      "--max-model-len=8192" 
    ]