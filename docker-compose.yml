version: "3.8"

services:
  vllm:
    # INFO: GPU not working with Turing on official images due to async/eager issues
    # CPU (problematic): public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.1
    # CPU (own build): vllm-cpu-env
    # CPU COMMAND: DOCKER_BUILDKIT=1 docker build --file docker/Dockerfile.cpu --tag vllm-cpu-env --target vllm-openai .
    image: vllm-cpu-env
    pull_policy: missing
    privileged: true
    # network_mode: host
    restart: unless-stopped
    environment:
      - VLLM_API_KEY=vllm-api-key
      - VLLM_LOGGING_LEVEL=DEBUG
      - VLLM_CPU_KVCACHE_SPACE=8
      - VLLM_TRACE_FUNCTION=1
    ports:
      # IMPORTANT: It cannot be 8000 for some reason
      - "63333:63333"
    volumes:
      - "~/.cache/huggingface:/root/.cache/huggingface"
    command: [
      # Setting custom port because of issues with 8000
      "--port=63333",
      # Serve all networks
      "--host=0.0.0.0",
      # Use CPU for inference
      "--device=cpu",
      # "Gemma is provided under and subject to the Gemma Terms of Use found at ai.google.dev/gemma/terms"
      "--model=gaunernst/gemma-3-4b-it-int4-awq",
      "--served-model-name=localmodel",
      # Use dtype for model precision
      "--dtype=bfloat16",
      # Possible reasoning choices (required for enable): deepseek_r1, granite
      # "--enable-reasoning",
      # "--reasoning-parser=deepseek_r1",
      # Use ngram spec-dec... free performance
      "--speculative-config={\"method\": \"ngram\", \"num_speculative_tokens\": 5, \"prompt_lookup_max\": 4}",
      # Quantize KV cache to FP8
      "--kv-cache-dtype=fp8",
      # Use outlines for best JSON Schema support
      "--guided-decoding-backend=outlines",
      # Cache prefix tokens for faster generation
      "--enable-prefix-caching",
      # Deterministic-ish outputs
      "--seed=666",
      # Use VLLM generation config over model
      "--generation-config=auto",
      "--override-generation-config={\"temperature\": 0}",
      # Model context length. If unspecified, will be automatically derived from the model config.
      "--max-model-len=8192",
      # Trust code embedded in pulled models 
      "--trust-remote-code",
      # Set API key to your API key here (mirrors environment variable)
      "--allow-credentials",
      "--api-key=vllm-api-key",
    ]
